{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88227be0-81df-4a62-88f2-47a012d127ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-21T20:38:04.024411Z",
     "iopub.status.busy": "2023-10-21T20:38:04.024124Z",
     "iopub.status.idle": "2023-10-21T20:44:32.281492Z",
     "shell.execute_reply": "2023-10-21T20:44:32.280700Z",
     "shell.execute_reply.started": "2023-10-21T20:38:04.024381Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preloading val data...: 100%|██████████| 10000/10000 [00:34<00:00, 292.71it/s]\n",
      "Preloading train data...: 100%|██████████| 100000/100000 [05:50<00:00, 285.61it/s]\n"
     ]
    }
   ],
   "source": [
    "from TinyImageNetLoader import TinyImageNetDataset\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(224, antialias=True),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "valset = TinyImageNetDataset(\"/datasets/tiny-imagenet-200\", mode=\"val\", transform=val_transform)\n",
    "#print(next(enumerate(validation_loader)))\n",
    "\n",
    "train_transform =  transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(224, antialias=True),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(20),\n",
    "    normalize\n",
    "])\n",
    "\n",
    "trainset = TinyImageNetDataset(\"/datasets/tiny-imagenet-200\", transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38d4233c-4ab7-4dea-abb7-b40126ea0890",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-21T20:44:34.198685Z",
     "iopub.status.busy": "2023-10-21T20:44:34.198294Z",
     "iopub.status.idle": "2023-10-21T20:44:34.203144Z",
     "shell.execute_reply": "2023-10-21T20:44:34.202382Z",
     "shell.execute_reply.started": "2023-10-21T20:44:34.198655Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_loader = torch.utils.data.DataLoader(\n",
    "        valset, batch_size=256, shuffle=False, num_workers=4)\n",
    "training_loader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=256, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "644a9abb-1831-4e19-b8d9-0126f6eef262",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-21T20:44:35.849758Z",
     "iopub.status.busy": "2023-10-21T20:44:35.849178Z",
     "iopub.status.idle": "2023-10-21T20:44:35.857984Z",
     "shell.execute_reply": "2023-10-21T20:44:35.857186Z",
     "shell.execute_reply.started": "2023-10-21T20:44:35.849734Z"
    }
   },
   "outputs": [],
   "source": [
    "eps=1e-10\n",
    "def loss_fn(out, labels, predicate_matrix):\n",
    "    out = out.view(-1, 1, NUM_FEATURES) # out is a batch of 1D binary vectors\n",
    "    ANDed = out * predicate_matrix # AND operation\n",
    "    diff = ANDed - out # Difference of ANDed and out => if equal, then out is a subset of its class' predicates\n",
    "\n",
    "    entr_loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    loss_cl = entr_loss(diff.sum(dim=2), labels) # Is \"out\" a subset of its class' predicates?\n",
    "\n",
    "    batch_size = out.shape[0]\n",
    "\n",
    "    out = out.view(-1, NUM_FEATURES)\n",
    "    diff_square = (out - predicate_matrix[labels]).pow(2)\n",
    "    \n",
    "    false_positives = (out - predicate_matrix[labels] + diff_square).sum() / batch_size\n",
    "    missing_attr = (predicate_matrix[labels] - out + diff_square).sum() / batch_size\n",
    "    \n",
    "    loss_ft = (1 + false_positives + missing_attr)\n",
    "    loss_ft *= loss_cl.item()/(loss_ft.item() + eps)\n",
    "    \n",
    "    #loss_mean_attr = (predicate_matrix.sum(dim=1).mean() - NUM_FEATURES//2).pow(2)\n",
    "    #loss_mean_attr *= loss_cl.item()/(loss_mean_attr.item() + eps)\n",
    "    \n",
    "    return loss_cl + loss_ft * FT_WEIGHT\n",
    "\n",
    "import sys, os\n",
    "sys.path.insert(0, \"/\".join(os.path.abspath('').split(\"/\")[:-1]) + \"/ZSL\")\n",
    "from sam import SAM\n",
    "def train_one_epoch(scheduler):\n",
    "    running_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data[\"images\"], data[\"labels\"]\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs, commit_loss, predicate_matrix = model(inputs)\n",
    "        loss = loss_fn(outputs, labels, predicate_matrix) + commit_loss\n",
    "        \n",
    "        # first forward-backward pass\n",
    "        loss.backward()\n",
    "        optimizer.first_step(zero_grad=True)\n",
    "\n",
    "        outputs, commit_loss, predicate_matrix = model(inputs)\n",
    "        loss = loss_fn(outputs, labels, predicate_matrix) + commit_loss\n",
    "        \n",
    "        # second forward-backward pass\n",
    "        loss.backward()  # make sure to do a full forward pass\n",
    "        optimizer.second_step(zero_grad=True)\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "    \n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    return running_loss / (i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d70d4ca-bb78-4a1e-a9fc-2fcc10247d83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-21T20:57:32.515378Z",
     "iopub.status.busy": "2023-10-21T20:57:32.514967Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 6/50 [1:11:26<8:43:42, 714.14s/it, LOSS: 10.275333952903747, ACC: 0.010644531808793545, FP: 6.934375286102295, MA: 21.02949333190918, OA: 10.59531307220459] "
     ]
    }
   ],
   "source": [
    "sys.path.insert(0, \"/\".join(os.path.abspath('').split(\"/\")[:-1]) + \"/models\")\n",
    "from DeiT3AutoPredicates import ResExtr\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "NUM_FEATURES = 80\n",
    "NUM_CLASSES = 200\n",
    "EPOCHS = 50\n",
    "accuracy = Accuracy(task=\"multiclass\", num_classes=NUM_CLASSES, top_k=1).to(device)\n",
    "\n",
    "FT_WEIGHT = 0.7\n",
    "\n",
    "model = ResExtr(NUM_FEATURES, NUM_CLASSES, deit_type=1, pretrained=True).to(device)\n",
    "\n",
    "base_optimizer = torch.optim.Adam\n",
    "optimizer = SAM(model.parameters(), base_optimizer, lr=3e-5, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer.base_optimizer, 1e-4, epochs=EPOCHS, steps_per_epoch=len(training_loader))\n",
    "#scheduler = None\n",
    "\n",
    "best_stats = {\n",
    "    \"epoch\": 0,\n",
    "    \"train_loss\": 0,\n",
    "    \"val_loss\": 0,\n",
    "    \"val_acc\": 0,\n",
    "    \"fp\": 0,\n",
    "    \"ma\": 0,\n",
    "    \"oa\": 0\n",
    "}\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "pbar = tqdm(range(EPOCHS))\n",
    "\n",
    "for epoch in pbar:\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(scheduler)\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "    running_acc = 0.0\n",
    "    running_false_positives = 0.0\n",
    "    running_missing_attr = 0.0\n",
    "    running_out_attributes= 0.0\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata[\"images\"], vdata[\"labels\"]\n",
    "            vinputs = vinputs.to(device)\n",
    "            vlabels = vlabels.to(device)\n",
    "            voutputs, vcommit_loss, predicate_matrix = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels, predicate_matrix) + vcommit_loss\n",
    "            running_vloss += vloss.item()\n",
    "            voutputs = voutputs.view(-1, 1, NUM_FEATURES)\n",
    "            ANDed = voutputs * predicate_matrix\n",
    "            diff = ANDed - voutputs\n",
    "            running_acc += accuracy(diff.sum(dim=2), vlabels)\n",
    "            voutputs = voutputs.view(-1, NUM_FEATURES)\n",
    "            running_false_positives += ((predicate_matrix[vlabels] - voutputs) == -1).sum() / voutputs.shape[0]\n",
    "            running_missing_attr += ((voutputs - predicate_matrix[vlabels]) == -1).sum() / voutputs.shape[0]\n",
    "            running_out_attributes += voutputs.sum() / voutputs.shape[0]\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    avg_acc = running_acc / (i + 1)\n",
    "    avg_fp = running_false_positives / (i + 1)\n",
    "    avg_ma = running_missing_attr / (i + 1)\n",
    "    avg_oa = running_out_attributes / (i + 1)\n",
    "    pbar.set_postfix_str(f\"LOSS: {avg_vloss}, ACC: {avg_acc}, FP: {avg_fp}, MA: {avg_ma}, OA: {avg_oa}\")\n",
    "    \n",
    "    if best_stats[\"val_acc\"] < avg_acc:\n",
    "        best_stats[\"epoch\"] = epoch\n",
    "        best_stats[\"train_loss\"] = avg_loss\n",
    "        best_stats[\"val_loss\"] = avg_vloss\n",
    "        best_stats[\"val_acc\"] = avg_acc.item()\n",
    "        best_stats[\"fp\"] = avg_fp.item()\n",
    "        best_stats[\"ma\"] = avg_ma.item()\n",
    "        best_stats[\"oa\"] = avg_oa.item()\n",
    "\n",
    "\n",
    "print(best_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
